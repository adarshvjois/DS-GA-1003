{
 "metadata": {
  "name": "",
  "signature": "sha256:ef784566033899e7a176bad92de7f496418e1a4be84e2274bfa1e6367b00d413"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import logging\n",
      "import numpy as np\n",
      "import sys\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.cross_validation import train_test_split\n",
      "import time\n",
      "from mpl_toolkits.mplot3d.axes3d import Axes3D"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2 Linear Regression\n",
      "## 2.1 Min-Max scaling\n",
      "The following is my implementation of Min-Max scaling. I referred to some scikit-learn documentation while doing so in order to handle some cases.\n",
      "\\begin{equation*}\n",
      "X^{i,j} = \n",
      "\\begin{cases}\n",
      "\\frac{X^{i,j} - X^j_{min}}{X^j_{max} - X^j_{min}} & \\text{if } X^j_{max} \\neq X^j_{min} \\neq 0 \\\\\n",
      "1/c & \\text{if } X^j_{max} = X^j_{min} = c \\\\\n",
      "0 & \\text{otherwise}\\\\\n",
      "\\end{cases}\n",
      "\\end{equation*}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_normalization(train, test):\n",
      "    \"\"\"Rescales the data so that each feature in the training set is in\n",
      "    the interval [0,1], and apply the same transformations to the test\n",
      "    set, using the statistics computed on the training set.\n",
      "    \n",
      "    Args:\n",
      "        train - training set, a 2D numpy array of size (num_instances, num_features)\n",
      "        test  - test set, a 2D numpy array of size (num_instances, num_features)\n",
      "    Returns:\n",
      "        train_normalized - training set after normalization\n",
      "        test_normalized  - test set after normalization\n",
      "\n",
      "    \"\"\"\n",
      "    for j in range(1, train.shape[1]):\n",
      "        # X^j_max != X^j_min != 0\n",
      "        if train[: , j].min() == train[:, j].max() != 0:\n",
      "            train[:, j] *= (1.0 / train[:, j].max())\n",
      "            test[:, j] = test[:, j] * (1.0 / train[:, j].max())\n",
      "            \n",
      "        if train[:, j].min() != train[:, j].max():\n",
      "            train[:, j] = (train[:, j] - train[:, j].min()) / (train[:, j].max() - train[:, j].min())\n",
      "            test[:, j] = (test[:, j] - train[:, j].min()) / (train[:, j].max() - train[:, j].min())\n",
      "\n",
      "    return train, test\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2.1 Loading data and Normalization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('loading the dataset')\n",
      "    \n",
      "df = pd.read_csv('hw1-data.csv', delimiter=',')\n",
      "X = df.values[:, :-1]\n",
      "y = df.values[:, -1]\n",
      "    \n",
      "print('Split into Train and Test')\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=10)\n",
      " \n",
      "print(\"Scaling all to [0, 1]\")\n",
      "X_train, X_test = feature_normalization(X_train, X_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading the dataset\n",
        "Split into Train and Test\n",
        "Scaling all to [0, 1]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2.2 Gradient Descent Setup\n",
      "### 1.\n",
      "The equation for gradient descent can be written in matrix form as:\n",
      "\\begin{equation*}\n",
      "J(\\theta) = \\frac{1}{2m}(X\\theta - Y)^T(X\\theta - Y)\n",
      "\\end{equation*}\n",
      "\n",
      "We can solve for the gradient of this expression:\n",
      "\\begin{align*}\n",
      "J(\\theta) &= \\frac{1}{2m}(\\theta^TX^T - Y^T)(X\\theta - Y)\\\\\n",
      "&=\\frac{1}{2m}(\\theta^TX^TX\\theta - \\theta^TX^TY -Y^TX\\theta -Y^TY)\\\\\n",
      "&=\\frac{1}{2m}(\\theta^TX^TX\\theta - 2Y^TX\\theta -Y^TY)\\\\\n",
      "\\end{align*}\n",
      "###2.\n",
      "Since $J(\\theta)$ is a scalar all the terms in this expression are constants. Therefore we can say that: $\n",
      "\\theta^TX^TY = Y^TX\\theta $ since it is clear that one the transpose of the other and the transpose of a scalar is itself\n",
      "\n",
      "\\begin{align*}\n",
      "\\nabla_\\theta J(\\theta) &= \\frac{1}{2m} (2X^TX\\theta - 2YX^T)\\\\\n",
      "&=\\frac{1}{m}X^T(X\\theta - Y)\n",
      "\\end{align*}\n",
      "\n",
      "###3.\n",
      "We can use the gradient as an approximation of the function itself for small increments in the parameter. Since the unit vector specifies the direction in which we want this increment the projection of the gradient in direction of the unit vector is what we require.\n",
      "\\begin{equation*}\n",
      "J(\\theta + \\eta\\nabla) - J(\\theta) \\approx \\eta\\nabla^T\\theta\n",
      "\\end{equation*}\n",
      "\n",
      "###4.\n",
      "The equation to update $\\theta$ is:\n",
      "$\n",
      "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J\n",
      "$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###5. Compute square loss"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_square_loss(X, y, theta):\n",
      "    \"\"\"\n",
      "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        theta - the parameter vector, 1D array of size (num_features)\n",
      "    \n",
      "    Returns:\n",
      "        loss - the square loss, scalar\n",
      "    \"\"\"\n",
      "    \n",
      "    loss = np.dot(X, theta) - y\n",
      "    return 0.5 * np.sum(loss ** 2) / X.shape[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6. Compute gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_square_loss_gradient(X, y, theta):\n",
      "    \"\"\"\n",
      "    Compute gradient of the square loss (as defined in compute_square_loss), at the point theta.\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
      "    \n",
      "    Returns:\n",
      "        grad - gradient vector, 1D numpy array of size (num_features)\n",
      "    \"\"\"\n",
      "    # TODO\n",
      "    num_instances = X.shape[0]\n",
      "    loss = np.dot(X, theta) - y\n",
      "    theta = np.dot(X.T, loss)\n",
      "    return (1.0 / num_instances) * theta\n",
      "       \n",
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2.3 Gradient checker\n",
      "To numerically check the gradient at a point we can use the following:\n",
      "\\begin{equation*}\n",
      "lim_{\\epsilon \\rightarrow 0}\\frac{J(\\theta + \\epsilon\\theta) -J(\\theta - \\epsilon\\theta)}{2\\epsilon}\n",
      "\\end{equation*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###1.\n",
      "Gradient for batch gradient descent:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4): \n",
      "    \"\"\"Implement Gradient Checker\n",
      "    Check that the function compute_square_loss_gradient returns the\n",
      "    correct gradient for the given X, y, and theta.\n",
      "\n",
      "    Let d be the number of features. Here we numerically estimate the\n",
      "    gradient by approximating the directional derivative in each of\n",
      "    the d coordinate directions: \n",
      "    (e_1 = (1,0,0,...,0), e_2 = (0,1,0,...,0), ..., e_d = (0,...,0,1) \n",
      "\n",
      "    The approximation for the directional derivative of J at the point\n",
      "    theta in the direction e_i is given by: \n",
      "    ( J(theta + epsilon * e_i) - J(theta - epsilon * e_i) ) / (2*epsilon).\n",
      "\n",
      "    We then look at the Euclidean distance between the gradient\n",
      "    computed using this approximation and the gradient computed by\n",
      "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
      "    distance exceeds tolerance, we say the gradient is incorrect.\n",
      "\n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
      "        epsilon - the epsilon used in approximation\n",
      "        tolerance - the tolerance error\n",
      "    \n",
      "    Return:\n",
      "        A boolean value indicate whether the gradient is correct or not\n",
      "\n",
      "    \"\"\"\n",
      "    true_gradient = compute_square_loss_gradient(X, y, theta)  # the true gradient\n",
      "    num_features = theta.shape[0]\n",
      "    approx_grad = np.zeros(num_features)  # Initialize the gradient we approximate\n",
      "    # TODO\n",
      "    for i in range(num_features):\n",
      "        \n",
      "        theta_plus = theta.copy();\n",
      "        theta_plus[i] = theta_plus[i] + epsilon\n",
      "        theta_minus = theta.copy()\n",
      "        theta_minus[i] = theta_minus[i] - epsilon\n",
      "        approx_grad[i] = compute_square_loss(X, y, theta_plus) - compute_square_loss(X, y, theta_minus)\n",
      "        approx_grad[i] /= (2.0 * epsilon)\n",
      "    \n",
      "    return np.sqrt(np.sum((true_gradient - approx_grad) ** 2)) < tolerance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2.\n",
      "Generic gradient checker:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, epsilon=0.01, tolerance=1e-4):\n",
      "    \"\"\"\n",
      "    The functions takes objective_func and gradient_func as parameters. And check whether gradient_func(X, y, theta) returned\n",
      "    the true gradient for objective_func(X, y, theta).\n",
      "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
      "    \"\"\"\n",
      "    # TODO\n",
      "    true_gradient = compute_square_loss_gradient(X, y, theta)  # the true gradient\n",
      "    num_features = theta.shape[0]\n",
      "    approx_grad = np.zeros(num_features)  # Initialize the gradient we approximate\n",
      "    # TODO\n",
      "    for i in range(num_features):\n",
      "        \n",
      "        theta_plus = theta.copy();\n",
      "        theta_plus[i] = theta_plus[i] + epsilon\n",
      "        theta_minus = theta.copy()\n",
      "        theta_minus[i] = theta_minus[i] - epsilon\n",
      "        approx_grad[i] = compute_square_loss(X, y, theta_plus) - compute_square_loss(X, y, theta_minus)\n",
      "        approx_grad[i] /= (2.0 * epsilon)\n",
      "    \n",
      "    return np.sqrt(np.sum((true_gradient - approx_grad) ** 2)) < tolerance\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2.4 Batch Gradient descent\n",
      "###1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def batch_grad_descent(X, y, alpha=0.1, num_iter=1000, check_gradient=False):\n",
      "    \"\"\"\n",
      "    In this question you will implement batch gradient descent to\n",
      "    minimize the square loss objective\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        alpha - step size in gradient descent\n",
      "        num_iter - number of iterations to run \n",
      "        check_gradient - a boolean value indicating whether checking the gradient when updating\n",
      "        \n",
      "    Returns:\n",
      "        theta_hist - store the the history of parameter vector in iteration, 2D numpy array of size (num_iter+1, num_features) \n",
      "                    for instance, theta in iteration 0 should be theta_hist[0], theta in ieration (num_iter) is theta_hist[-1]\n",
      "        loss_hist - the history of objective function vector, 1D numpy array of size (num_iter+1) \n",
      "    \"\"\"\n",
      "    num_instances, num_features = X.shape[0], X.shape[1]\n",
      "    theta_hist = np.zeros((num_iter + 1, num_features))  # Initialize theta_hist\n",
      "    loss_hist = np.zeros(num_iter + 1)  # initialize loss_hist\n",
      "    theta = np.ones(num_features)  # initialize theta\n",
      "    # TODO\n",
      "    \n",
      "    t0 = time.time()\n",
      "\n",
      "    for i in xrange(num_iter):\n",
      "        \n",
      "        loss_hist[i] = compute_square_loss(X, y, theta)\n",
      "        theta_hist[i] = theta\n",
      "        grad = compute_square_loss_gradient(X, y, theta) \n",
      "               \n",
      "        if(grad_checker == True):\n",
      "            if(grad_checker(X, y, theta)):\n",
      "                logging.warn(\"Gradient estimated seems to deviate too far from what it should be\")\n",
      "            else:\n",
      "                logging.info(\"Life's good.\")\n",
      "\n",
      "        theta = theta - alpha * grad\n",
      "    t1 = time.time()\n",
      "\n",
      "    #print \"Average time per iter:: \" + str((t1 - t0) / num_iter)\n",
      "    return theta_hist, loss_hist\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2.\n",
      "As the step sizes get smaller we see the convergence rate reduces.\n",
      "Convergence plots for various step sizes\n",
      "<img src = \"step_size_vs_iter.png\" height=\"300\" width=\"450\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convergence_tests(X, y):\n",
      "    num_iter = 500\n",
      "    \n",
      "    alpha = np.array([0.0005,0.005,0.001,0.01])\n",
      "    loss_vs_alpha = np.zeros(len(alpha))\n",
      "    plt.subplot(122)\n",
      "    for i in range(len(alpha)):\n",
      "        [theta, losses] = batch_grad_descent(X, y, alpha[i],num_iter)\n",
      "        plt.plot(np.log(losses), label=\"Alpha(Batch Grad)\" + str(alpha[i]))\n",
      "    plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "convergence_tests(X_train,y_train)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2.5 Ridge Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###1. Gradient of the regularized cost function:\n",
      "The cost function in matrix form can be written as:\n",
      "\\begin{align*}\n",
      "J(\\theta) &= \\frac{1}{2m} \\{(X\\theta - Y)^T(X\\theta-Y)\\} + \\lambda \\theta^T\\theta \\\\\n",
      "&= \\frac{1}{2m} \\{\\theta^TX^TX\\theta - 2Y^TX\\theta -Y^TY\\} + \\lambda \\theta^T\\theta\n",
      "\\end{align*}\n",
      "Differentiating this equation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align*}\n",
      "\\nabla_\\theta J(\\theta) &= \\frac{1}{2m} (2X^TX\\theta - 2YX^T) + 2\\lambda \\theta\\\\\n",
      "&=\\frac{1}{m}X^T(X\\theta - Y) + 2\\lambda \\theta\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The equation to update theta is: \n",
      "\\begin{equation*}\n",
      "\\theta \\leftarrow \\theta - \\eta \\frac{1}{m}X^T(X\\theta - Y) + 2\\lambda \\theta\n",
      "\\end{equation*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2. Compute regularized square loss gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
      "    \"\"\"\n",
      "    Computes the gradient of L2-regularized square loss function given X, y and theta\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
      "        lambda_reg - the regularization coefficient\n",
      "    \n",
      "    Returns:\n",
      "        grad - gradient vector, 1D numpy array of size (num_features)\n",
      "    \"\"\"\n",
      "    # TODO\n",
      "    regularization_term = 2.0 * lambda_reg * theta\n",
      "    \n",
      "    grad = compute_square_loss_gradient(X, y, theta) + regularization_term\n",
      "    return grad\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. Regularized gradient descent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def regularized_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
      "    \"\"\"\n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        alpha - step size in gradient descent\n",
      "        lambda_reg - the regularization coefficient\n",
      "        numIter - number of iterations to run \n",
      "        \n",
      "    Returns:\n",
      "        theta_hist - the history of parameter vector, 2D numpy array of size (num_iter+1, num_features) \n",
      "        loss_hist - the history of regularized loss value, 1D numpy array\n",
      "    \"\"\"\n",
      "    (num_instances, num_features) = X.shape\n",
      "    theta = np.ones(num_features)  # Initialize theta\n",
      "    theta_hist = np.zeros((num_iter + 1, num_features))  # Initialize theta_hist\n",
      "    loss_hist = np.zeros(num_iter + 1)  # Initialize loss_hist\n",
      "    # TODO\n",
      "    t0 = time.time()\n",
      "\n",
      "    for i in range(num_iter + 1):\n",
      "        loss_hist[i] = compute_square_loss(X, y, theta) + lambda_reg * np.sum(theta ** 2)\n",
      "        theta_hist[i] = theta\n",
      "\n",
      "        grad = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
      "        theta = theta - alpha * grad\n",
      "    \n",
      "    t1 = time.time()\n",
      "    # print \"Average time per step:: \"+str((t1 - t0)/num_iter) \n",
      "    return theta_hist, loss_hist\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###4. Bias dimension:\n",
      "Regularizing the bias dimension can hurt if we do not have data that is centered or normalized. In such a case the fit to the data may have a high bias term if a large offset is needed to fit the best hyperplane. Using a large value $B$ for the bias dimension means that the zeroth dimension of the data is some large value that contributes to the square loss error.\n",
      "\n",
      "Since the optimization objective is to find the minimum of the square loss error, the gradient descent algorithm seeks to minimize the contribution of the error from the bias term by driving the coefficient of the weight vector associated with it to a small value. i.e. as  $B\\rightarrow \\infty, \\theta_0 \\rightarrow 0$. As a side effect this also makes the convergence time for gradient descent higher."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###5. Visualizing the training and validation loss with respect to the regularization param.\n",
      "For this plot $\\lambda$ is varied over a range of values ranging from $2^{-10}$ to $2^5$.\n",
      "Values of step size and number of iterations were chosen such that convergence was achieved at all values of $\\lambda$.\n",
      "The value of B was chosen as 1.\n",
      "<img src=\"Lambda_vs_error1.png\" height=\"300\" width=\"450\">\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def visualize_loss_vs_lambda(X_train, X_test, y_train, y_test):\n",
      "    \n",
      "    X_train[:, -1] = np.ones(X_train.shape[0])\n",
      "    X_train[:, -1] = 1 * X_train[:, -1]\n",
      "    X_test[:, -1] = np.ones(X_test.shape[0])\n",
      "    X_test[:, -1] = 1 * X_test[:, -1]\n",
      "    \n",
      "    lambdas = np.arange(1, 8 , 0.25)\n",
      "    train_error = np.zeros(lambdas.shape[0])\n",
      "    validation_error = np.zeros(lambdas.shape[0])\n",
      "    i = 0\n",
      "    for l in lambdas:\n",
      "        [thetas, losses] = regularized_grad_descent(X_train, y_train, alpha=0.0001 *2.0/ 3, lambda_reg=l, num_iter=10000)\n",
      "        train_error[i] = compute_square_loss(X_train, y_train, thetas[-1])\n",
      "        validation_error[i] = compute_square_loss(X_test, y_test, thetas[-1])\n",
      "        i += 1\n",
      "    print \"Min lambda:: \" + str(lambdas[np.argmin(validation_error)])\n",
      "    print \"Min validation error::\" + str(validation_error.min())\n",
      "    print \"Min training error:: \" + str(train_error.min())\n",
      "    plt.plot(np.log(lambdas), train_error,label=\"Training error\", c='b')\n",
      "    plt.plot(np.log(lambdas), validation_error,label=\"Validation Error\" ,c='r')\n",
      "    plt.legend()\n",
      "    plt.xlabel(\"Lambda (log)\")\n",
      "    plt.ylabel(\"Error\")\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "visualize_loss_vs_lambda(X_train, X_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Min lambda:: 4.5\n",
        "Min validation error::2.4703405144\n",
        "Min training error:: 3.63392699151\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tuning this with respect to $\\lambda$ and $B$ a cross validation vs. training error plot with respect to these parameters is shown below:\n",
      "<img src=\"B_vs_Lambda3.png\">\n",
      "The code that was run to attain these plots is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parameter_search(X_train, X_test, y_train, y_test):\n",
      "    lambdas = 2.0 ** np.arange(-2, 8, 0.5)\n",
      "    B = np.arange(1, 5.5, 0.5)\n",
      "    train_error = np.zeros((B.shape[0], lambdas.shape[0]))\n",
      "    validation_error = np.zeros((B.shape[0], lambdas.shape[0]))\n",
      "    num_iter = 10000\n",
      "    i = 0\n",
      "    j = 0\n",
      "    \n",
      "    \n",
      "    \n",
      "    t0 = time.time()\n",
      "    for lambda_reg in lambdas:\n",
      "        j = 0    \n",
      "        for b in B:\n",
      "            X_train[:, -1] = np.ones(X_train.shape[0])\n",
      "            X_train[:, -1] = b * X_train[:, -1]\n",
      "            X_test[:, -1] = np.ones(X_test.shape[0])\n",
      "            X_test[:, -1] = b * X_test[:, -1]\n",
      "            [thetas, losses] = regularized_grad_descent(X_train, y_train,0.001, lambda_reg, num_iter=num_iter)\n",
      "            theta_best = thetas[-1]\n",
      "            train_error[j, i] = compute_square_loss(X_train, y_train, theta_best)\n",
      "            validation_error[j, i] = compute_square_loss(X_test, y_test, theta_best)\n",
      "            j += 1\n",
      "        i += 1\n",
      "    t1 = time.time()\n",
      "    \n",
      "    print \"Time to execute::\" + str(t1 - t0)\n",
      "    A = np.unravel_index(np.argmin(validation_error), validation_error.shape)\n",
      "    print \"Minimum cv error:: \" + str(validation_error.min())\n",
      "    print \"Minimum train error:: \" + str(train_error.min()) \n",
      "    print \"Value of B, Lambda at lowest cross validation error: \" + str(B[A[0]]) + \" , \" + str(lambdas[A[1]])\n",
      "    \n",
      "    # plt.plot(np.log(lambdas),train_error[:,0],'k',np.log(lambdas),validation_error[:,0],'r')\n",
      "    (L, B_S) = np.meshgrid(lambdas, B)\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
      "    ax.plot_surface(L, B_S, train_error, rstride=1, cstride=1, cmap='cool', linewidth=0, antialiased=False, alpha=0.25)\n",
      "    ax.set_xlabel('lambda (Log)')\n",
      "    ax.set_ylabel('B ')\n",
      "    ax.set_zlabel('train error)')\n",
      "    \n",
      "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
      "    ax.plot_surface(L, B_S, validation_error, rstride=1, cstride=1, cmap='cool', linewidth=0, antialiased=False, alpha=0.25)\n",
      "    ax.set_xlabel('lambda (Log)')\n",
      "    ax.set_ylabel('B ')\n",
      "    ax.set_zlabel('validation error)')\n",
      "    \n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The large number of iterations is to ensure convergence at higher values of B. For this dataset the regularizing the bias made no significant difference. Best value of $\\lambda$ and $B$ was observed at around,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parameter_search(X_train, X_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time to execute::82.9474339485\n",
        "Minimum cv error:: 2.46664399952\n",
        "Minimum train error:: 3.11157403722\n",
        "Value of B, Lambda at lowest cross validation error: 1.0 , 4.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/adarsh/anaconda/lib/python2.7/site-packages/mpl_toolkits/mplot3d/axes3d.py:1094: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
        "  if self.button_pressed in self._rotate_btn:\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###6. Average time\n",
      "Average time it took for a step of gradient descent was:\n",
      "$T_{avg} = 4.817053079605\\times 10^{-5} \\text{sec}$\n",
      "This was averaged over 100 runs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###7. Theta selection:\n",
      "The theta that we can select for deployment would be the one which shows the least error on the test data. This means that for these parameters this is the model that generalizes the best."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#2.4 Stochastic Gradient Descent:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###1. Update rule:\n",
      "The update rule for the SGD is the following:\n",
      "\\begin{align*}\n",
      "&\\text{Repeat until a minimum is reached:}\\\\\n",
      "&\\text{Repeat for all m samples:}\\\\\n",
      "&\\theta_{k,i} \\leftarrow \\theta_{k,i} - \\alpha \\nabla_\\theta J_i(\\theta)\n",
      "\\end{align*}\n",
      "Where $J_i(\\theta)$ is the regularized cost function for the $i^{th}$ training example and $k^{th}$ epoch"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2. Stochastic Gradient Descent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#############################################\n",
      "# ##Q2.6a: Stochastic Gradient Descent\n",
      "def stochastic_grad_descent(X, y, alpha=0.001, lambda_reg=1, num_iter=1000):\n",
      "    \"\"\"\n",
      "    In this question you will implement stochastic gradient descent with a regularization term\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        alpha - string or float. step size in gradient descent\n",
      "                NOTE: In SGD, it's not always a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
      "                if alpha is a float, then the step size in every iteration is alpha.\n",
      "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t)\n",
      "                if alpha == \"1/t\", alpha = 1/t\n",
      "        lambda_reg - the regularization coefficient\n",
      "        num_iter - number of epochs (i.e number of times) to go through the whole training set\n",
      "    \n",
      "    Returns:\n",
      "        theta_hist - the history of parameter vector, 3D numpy array of size (num_iter, num_instances, num_features) \n",
      "        loss hist - the history of regularized loss function vector, 2D numpy array of size(num_iter, num_instances)\n",
      "    \"\"\"\n",
      "    num_instances, num_features = X.shape[0], X.shape[1]\n",
      "    theta = np.ones(num_features)  # Initialize theta\n",
      "    \n",
      "    \n",
      "    theta_hist = np.zeros((num_iter, num_instances, num_features))  # Initialize theta_hist\n",
      "    loss_hist = np.zeros((num_iter, num_instances))  # Initialize loss_hist\n",
      "    # TODO\n",
      "    if isinstance(alpha, str):\n",
      "        if alpha == '1/t':\n",
      "            f = lambda x: 1.0 / x\n",
      "        elif alpha == '1/sqrt(t)':\n",
      "            f = lambda x: 1.0 / np.sqrt(x)\n",
      "        alpha = 0.01\n",
      "    elif isinstance(alpha, float):\n",
      "        f = lambda x: 1\n",
      "    else:\n",
      "        return\n",
      "\n",
      "    t0 = time.time()\n",
      "\n",
      "    for t in range(num_iter):\n",
      "        \n",
      "        \n",
      "        for i in range(num_instances):\n",
      "            gamma_t = alpha * f((i+1)*(t+1))\n",
      "\n",
      "            theta_hist[t , i] = theta\n",
      "            # compute loss for current theta\n",
      "            loss = np.dot(X[i], theta) - y[i]\n",
      "            # reg. term\n",
      "            regulariztion_loss = lambda_reg * np.dot(theta.T,theta)\n",
      "            # squared loss\n",
      "            loss_hist[t, i] = (0.5) * (loss) ** 2 + regulariztion_loss \n",
      "            \n",
      "            regularization_penalty = 2.0 * lambda_reg * theta \n",
      "            grad = X[i] * (loss) + regularization_penalty\n",
      "            theta = theta - gamma_t * grad\n",
      "                        \n",
      "    t1 = time.time()\n",
      "    #print \"Average time per epoch:: \" + str((t1 - t0) / num_iter) \n",
      "    return theta_hist, loss_hist\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. Plotting step sizes for convergence rate tests\n",
      "The varying of step sizes causes convergence to get faster or slower depending on the value of the parameter. The smaller the step size the slower the convergence rate as observed. Stochastic gradient descent is seen to oscillate around a minimum at the end of each epoch. Iterations are on the X-Axis and the Y-Axis is the Logarithm of the objective\n",
      "<img src=\"Batch_vs_stochastic.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convergence_tests_batch_vs_stochastic(X,y):\n",
      "    alphas = ['1/t','1/sqrt(t)',0.0005,0.001]\n",
      "    plt.subplot(121)\n",
      "\n",
      "    for alpha in alphas:\n",
      "        [thetas,losses] = stochastic_grad_descent(X, y, alpha, 5.67, 5)\n",
      "        plt.plot(np.log(losses.ravel()),label='Alpha:'+str(alpha))\n",
      "        \n",
      "    plt.legend()\n",
      "    convergence_tests(X, y)\n",
      "    plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "convergence_tests_batch_vs_stochastic(X_train,y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###4.\n",
      "Average time for an epoch of SGD is around  0.0020361443162. This was averaged over 100 runs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3.Risk Minimization\n",
      "###1. Decision Rule for Square loss function\n",
      "If a function $f^*$ minimizes the posterior risk, then $f^*$ is said to be a Bayes rule.\n",
      "\n",
      "Posterior risk is given by:\n",
      "\\begin{equation*}\n",
      "r(\\hat{y}|x) = E_{p(y|x)}[L(y,\\hat{y})]\n",
      "\\end{equation*}\n",
      "where $\\hat{y} = f(x)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our objective is to find $\\delta(x)$ where:\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can write the risk function as a conditional expectation in the following way:\n",
      "\n",
      "\\begin{align*}\n",
      "r(\\hat{y}|x) &= E[(y-\\hat{y})^2|X] \\\\\n",
      "&= E[y^2-2y\\hat{y}+\\hat{y}^2|X]\\\\\n",
      "&= E[y^2|X] - 2\\hat{y}E[y|x] + \\hat{y}^2 &\\text{by linearity of expectation}\\\\\n",
      "\\frac{\\delta r(\\hat{y}|x)}{\\delta\\hat{y}} &= -2E[y|x] + 2\\hat{y} &\\text{differentiating with respect to $\\hat{y}$ and equating to zero}\\\\\n",
      "\\hat{y} &= E[y|x] &\\text{Hence proved}\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2. Risk Minimization for absolute error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align*}\n",
      "L(y,\\hat{y}) &= |y-\\hat{y}|\\\\\n",
      "r(\\hat{y}|x) &= E[|y-\\hat{y}|| x] &\\text{ Going by the prev. question this is our minimization objective}\\\\\n",
      "&= \\int |y-\\hat{y}| \\pi(y|x)dy &\\text{Definition of expectation}\\\\\n",
      "&= \\int_{y \\geq \\hat{y}}(y-\\hat{y})\\pi(y|x)dy + \\int_{y\\leq \\hat{y}}(\\hat{y}-y)\\pi(y|x)dy &\\text{splitting the domain of y} \\\\\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Differentiating the above equation on the broken domain of y and using differentiation under the integral sign:\n",
      "\\begin{align*}\n",
      "\\frac{\\delta r(\\hat{y}|x)}{\\delta\\hat{y}} &= -\\int_{y \\geq \\hat{y}}\\pi(y|x)dy + (\\hat{y}-\\hat{y})(1)-(\\hat{y}-y^+)(0)\n",
      "+\\int_{y\\leq \\hat{y}}\\pi(y|x)dy + (y^- - \\hat{y})(0)- (\\hat{y}-\\hat{y})(1)\\\\\n",
      "&=-\\int_{y \\geq \\hat{y}}\\pi(y|x)dy +\\int_{y\\leq \\hat{y}}\\pi(y|x)dy =0 &\\text{Equating to zero} \\\\\n",
      "\\int_{y \\geq \\hat{y}}\\pi(y|x)dy &= \\int_{y\\leq \\hat{y}}\\pi(y|x)dy &\\text{this is just the probability of..} \\\\\\\\\n",
      "P(y\\geq\\hat{y}) &= P(y\\leq \\hat{y})\n",
      "\\end{align*}\n",
      "\n",
      "This means that the value of $y$ can be the median value of $y$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}